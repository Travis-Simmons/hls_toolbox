{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/02_Data_Discovery_CMR-STAC_API.html\n",
    "\n",
    "from pystac_client import Client  \n",
    "from collections import defaultdict    \n",
    "import json\n",
    "import geopandas\n",
    "# import geoviews as gv\n",
    "from cartopy import crs\n",
    "# gv.extension('bokeh', 'matplotlib')\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find hls tiles given a point\n",
    "\n",
    "def find_hls_tiles(point=False, band=False, limit=False, collections = ['HLSL30.v2.0', 'HLSS30.v2.0'], date_range = False):\n",
    "\n",
    "    STAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "\n",
    "\n",
    "    catalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        x, y = point[0], point[1]\n",
    "        # print(x,y)\n",
    "    except TypeError:\n",
    "        print(\"Point must be in the form of [lon,lat]\")\n",
    "        raise\n",
    "\n",
    "    point = geopandas.points_from_xy([x],[y])\n",
    "    point = point[0]\n",
    "\n",
    "\n",
    "\n",
    "    # JOHN - THIS IS WHERE YOU WOULD ADD IN SEARCH PARAMETERS\n",
    "    if date_range:\n",
    "\n",
    "        search = catalog.search(\n",
    "            collections=collections, intersects = point, datetime=date_range)\n",
    "    else:\n",
    "        search = catalog.search(\n",
    "            collections=collections, intersects = point)\n",
    "\n",
    "\n",
    "\n",
    "    # print(f'{search.matched()} Tiles Found...')\n",
    "\n",
    "\n",
    "    item_collection = search.get_all_items()\n",
    "\n",
    "    if limit:\n",
    "        item_collection = item_collection[:limit]\n",
    "\n",
    "    if band:\n",
    "        links = []\n",
    "        if type(band) == list:\n",
    "            for i in item_collection:\n",
    "                for b in band:\n",
    "                    link = i.assets[b].href\n",
    "                    # print(link)\n",
    "                    links.append(link)\n",
    "        \n",
    "        else:\n",
    "            for i in item_collection:\n",
    "                link = i.assets[band].href\n",
    "                links.append(link)\n",
    "    \n",
    "    else:\n",
    "        links =[]\n",
    "        for i in item_collection:\n",
    "            # print(i.assets)\n",
    "            for key in i.assets:\n",
    "                if key.startswith('B'):\n",
    "                    # link = i.assets[key].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://')\n",
    "                    link = i.assets[key].href\n",
    "\n",
    "                    # print(link)\n",
    "                    links.append(link)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a reach ID, find the nodes\n",
    "\n",
    "import glob\n",
    "import netCDF4\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_dir = '/home/confluence/data/mnt/input/sword'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_reach_nodes(data_dir, reach_id):\n",
    "\n",
    "    all_nodes = []\n",
    "\n",
    "    files = glob.glob(os.path.join(data_dir, '*'))\n",
    "    print(f'Searching across {len(files)} continents for nodes...')\n",
    "\n",
    "    for i in files:\n",
    "\n",
    "        rootgrp = netCDF4.Dataset(i, \"r\", format=\"NETCDF4\")\n",
    "\n",
    "        node_ids_indexes = np.where(rootgrp.groups['nodes'].variables['reach_id'][:].data.astype('U') == str(reach_id))\n",
    "\n",
    "        if len(node_ids_indexes[0])!=0:\n",
    "            for y in node_ids_indexes[0]:\n",
    "                node_id = str(rootgrp.groups['nodes'].variables['node_id'][y].data.astype('U'))\n",
    "                all_nodes.append(node_id)\n",
    "\n",
    "\n",
    "\n",
    "            # all_nodes.extend(node_ids[0].tolist())\n",
    "\n",
    "        rootgrp.close()\n",
    "\n",
    "    print(f'Found {len(set(all_nodes))} nodes...')\n",
    "    return list(set(all_nodes))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get_reach_nodes(data_dir,74270100221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a reach ID, find the lat/lon points of all nodes\n",
    "\n",
    "\n",
    "\n",
    "import glob\n",
    "import netCDF4\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_dir = '/home/confluence/data/mnt/input/sword'\n",
    "\n",
    "\n",
    "def get_reach_node_cords(data_dir, reach_id):\n",
    "\n",
    "    all_nodes = []\n",
    "\n",
    "    files = glob.glob(os.path.join(data_dir, '*'))\n",
    "    print(f'Searching across {len(files)} continents for nodes...')\n",
    "\n",
    "    for i in files:\n",
    "\n",
    "        rootgrp = netCDF4.Dataset(i, \"r\", format=\"NETCDF4\")\n",
    "\n",
    "        node_ids_indexes = np.where(rootgrp.groups['nodes'].variables['reach_id'][:].data.astype('U') == str(reach_id))\n",
    "\n",
    "        if len(node_ids_indexes[0])!=0:\n",
    "            for y in node_ids_indexes[0]:\n",
    "\n",
    "                lat = str(rootgrp.groups['nodes'].variables['x'][y].data.astype('U'))\n",
    "                lon = str(rootgrp.groups['nodes'].variables['y'][y].data.astype('U'))\n",
    "                all_nodes.append([lat,lon])\n",
    "\n",
    "\n",
    "\n",
    "            # all_nodes.extend(node_ids[0].tolist())\n",
    "\n",
    "        rootgrp.close()\n",
    "\n",
    "    print(f'Found {len(all_nodes)} nodes...')\n",
    "    return all_nodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a reach ID, create download links for any hls tiles that intersect the nodes in the reach\n",
    "\n",
    "\n",
    "def find_download_links_for_reach_tiles(data_dir, reach_id):\n",
    "    node_coords = get_reach_node_cords(data_dir,reach_id)\n",
    "    all_links = []\n",
    "    for i in node_coords:\n",
    "        print(i)\n",
    "        links = find_hls_tiles(i,limit=1)\n",
    "        print(links)\n",
    "        all_links.extend(links)\n",
    "\n",
    "    return list(set(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('/data/data/downloads/Aqusat_TSS_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2001-06-04\n",
       "1        1992-06-02\n",
       "2        2002-06-05\n",
       "3        1992-08-19\n",
       "4        1984-09-18\n",
       "            ...    \n",
       "32141    2008-08-11\n",
       "32142    2017-10-16\n",
       "32143    2013-09-27\n",
       "32144    2016-07-18\n",
       "32145    2010-10-13\n",
       "Name: date, Length: 32146, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = df['date'];dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = \"2001-06-04T00:00:00Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon, date = str(df.iloc[100]['lat']), str(df.iloc[100]['long']), df.iloc[0]['date']\n",
    "i = [lat,lon]\n",
    "# i = ['-89.4447009885916', '37.4435521655434']\n",
    "# print(lat,lon,date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# max_count = 100\n",
    "\n",
    "\n",
    "# https://rafatieppo.github.io/post/2018_12_01_juliandate/\n",
    "import datetime\n",
    "\n",
    "def datestdtojd (stddate):\n",
    "    fmt='%Y-%m-%d'\n",
    "    sdtdate = datetime.datetime.strptime(stddate, fmt)\n",
    "    sdtdate = sdtdate.timetuple()\n",
    "    jdate = sdtdate.tm_yday\n",
    "    return(stddate[:4]+str(jdate))\n",
    "\n",
    "def jdtodatestd (jdate):\n",
    "    fmt = '%Y%j'\n",
    "    datestd = datetime.datetime.strptime(jdate, fmt).date()\n",
    "    out = datestd.strftime( '%Y-%m-%d')\n",
    "    return out\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('/data/data/downloads/Aqusat_TSS_v1.csv')\n",
    "\n",
    "\n",
    "def ssc_json_gen(aquasat_path):\n",
    "    ssc_dict = {}\n",
    "\n",
    "    # read in aquasat\n",
    "    df = pd.read_csv(aquasat_path)\n",
    "\n",
    "    f = IntProgress(min=0, max=len(df)) # instantiate the bar\n",
    "    display(f) # display the bar\n",
    "\n",
    "    fails = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        lat, lon, date = str(df.iloc[i]['lat']), str(df.iloc[i]['long']), df.iloc[0]['date']\n",
    "        try:\n",
    "            # date = datestdtojd('2013-01-13')\n",
    "            # print(date)\n",
    "            links = find_hls_tiles(point=[lon, lat])\n",
    "            if len(links)>0:\n",
    "                date_list = []\n",
    "                for l in links:\n",
    "                    date = l.split('/')[-1].split('.')[3][:7]\n",
    "                    date_list.append(date)\n",
    "\n",
    "                date_list = [jdtodatestd(x) for x in set(date_list)]\n",
    "\n",
    "                date_list.sort(key=lambda date: datetime.datetime.strptime(date, '%Y-%m-%d'))\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                date_dict = {}\n",
    "                for z in date_list:\n",
    "                    \n",
    "                    link_list_for_date = []\n",
    "                    for y in links:\n",
    "                        date = y.split('/')[-1].split('.')[3][:7]\n",
    "                        date = jdtodatestd(date)\n",
    "                        if date == z:\n",
    "                            link_list_for_date.append(y)\n",
    "                    \n",
    "                    # build_entry\n",
    "                    \n",
    "                    date_dict[z] = link_list_for_date\n",
    "\n",
    "                ssc_dict[str(lon) +','+ str(lat)]  = date_dict\n",
    "                # break\n",
    "                \n",
    "        except:\n",
    "            fails +=1\n",
    "        f.value += 1 # signal to increment the progress bar\n",
    "        time.sleep(.1)\n",
    "    print(f'Total fails: {fails} ')\n",
    "    print(f'Total Success: {len(df)-fails}')\n",
    "    return ssc_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476be2f060c44bd1b5287128a01c78c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=32146)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aqua_path = '/data/data/downloads/Aqusat_TSS_v1.csv'\n",
    "\n",
    "jj = ssc_json_gen(aqua_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T15TWN.2022316T165848.v2.0/HLS.L30.T15TWN.2022316T165848.v2.0.B05.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T15TWN.2022316T165848.v2.0/HLS.L30.T15TWN.2022316T165848.v2.0.B09.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T15TWN.2022316T165848.v2.0/HLS.L30.T15TWN.2022316T165848.v2.0.B07.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T15TWN.2022316T165848.v2.0/HLS.L30.T15TWN.2022316T165848.v2.0.B01.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T15TWN.2022316T165848.v2.0/HLS.L30.T15TWN.2022316T165848.v2.0.B03.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T15TWN.2022316T165848.v2.0/HLS.L30.T15TWN.2022316T165848.v2.0.B06.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T15TWN.2022316T165848.v2.0/HLS.L30.T15TWN.2022316T165848.v2.0.B04.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T15TWN.2022316T165848.v2.0/HLS.L30.T15TWN.2022316T165848.v2.0.B11.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T15TWN.2022316T165848.v2.0/HLS.L30.T15TWN.2022316T165848.v2.0.B10.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T15TWN.2022316T165848.v2.0/HLS.L30.T15TWN.2022316T165848.v2.0.B02.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B10.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B09.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B12.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B03.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B04.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B05.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B08.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B11.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B01.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B8A.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B07.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B06.tif',\n",
       " 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T15TWN.2022316T170539.v2.0/HLS.S30.T15TWN.2022316T170539.v2.0.B02.tif']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jj['-92.141332,47.529724']['2022-11-12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing json\n",
    "json_object = json.dumps(jj, indent=4)\n",
    " \n",
    "# Writing to sample.json\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# df = pd.read_csv('/data/data/downloads/Aqusat_TSS_v1.csv')\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     # print(i)\n",
    "#     lat, lon, date = str(df.iloc[i]['lat']), str(df.iloc[i]['long']), df.iloc[0]['date']\n",
    "#     try:\n",
    "#         #date not working\n",
    "#         date = \"2013-13-01T00:00:00Z/2014-13-01T00:00:00Z\" \n",
    "#         links = find_hls_tiles(point=[lon, lat])\n",
    "#         if len(links)>0:\n",
    "#             print(links)\n",
    "#             break\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1120 1104 1126 ... 2014 2082 2171]\n",
      "  [ 998  859  890 ... 1966 2051 2188]\n",
      "  [ 953  743  782 ... 2031 2088 2172]\n",
      "  ...\n",
      "  [3467 3394 3233 ... 3795 3576 3397]\n",
      "  [3542 3445 3266 ... 3767 3374 3346]\n",
      "  [3533 3497 3365 ... 3772 3607 3699]]]\n"
     ]
    }
   ],
   "source": [
    "# https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/05_Data_Access_Direct_S3.html\n",
    "\n",
    "# need to make netrc file\n",
    "\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "from rasterio.session import AWSSession\n",
    "from rasterio.plot import show\n",
    "import rioxarray\n",
    "# import geoviews as gv\n",
    "# import hvplot.xarray\n",
    "# import holoviews as hv\n",
    "# gv.extension('bokeh', 'matplotlib')\n",
    "\n",
    "\n",
    "s3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n",
    "\n",
    "def get_temp_creds():\n",
    "    temp_creds_url = s3_cred_endpoint\n",
    "    return requests.get(temp_creds_url).json()\n",
    "\n",
    "\n",
    "temp_creds_req = get_temp_creds()\n",
    "\n",
    "\n",
    "session = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n",
    "                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n",
    "                        aws_session_token=temp_creds_req['sessionToken'],\n",
    "                        region_name='us-west-2')\n",
    "\n",
    "\n",
    "rio_env = rio.Env(AWSSession(session),\n",
    "                  GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n",
    "                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\n",
    "\n",
    "            \n",
    "rio_env.__enter__()\n",
    "\n",
    "hls_da = rioxarray.open_rasterio(jj[\"-92.141332,47.529724\"][\"2013-05-11\"][0], chuncks=True)\n",
    "\n",
    "test = hls_da.values\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hlstutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ba854cefeea420105352005a7fd47290ef8212ad72b2335f23a5c2f1b863bf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
