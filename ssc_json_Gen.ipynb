{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSC JSON GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find hls tiles given a point\n",
    "\n",
    "def find_hls_tiles(point=False, band=False, limit=False, collections = ['HLSL30.v2.0', 'HLSS30.v2.0'], date_range = False):\n",
    "\n",
    "    STAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "\n",
    "\n",
    "    catalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        x, y = point[0], point[1]\n",
    "        # print(x,y)\n",
    "    except TypeError:\n",
    "        print(\"Point must be in the form of [lon,lat]\")\n",
    "        raise\n",
    "\n",
    "    point = geopandas.points_from_xy([x],[y])\n",
    "    point = point[0]\n",
    "\n",
    "\n",
    "\n",
    "    # JOHN - THIS IS WHERE YOU WOULD ADD IN SEARCH PARAMETERS\n",
    "    if date_range:\n",
    "\n",
    "        search = catalog.search(\n",
    "            collections=collections, intersects = point, datetime=date_range)\n",
    "    else:\n",
    "        search = catalog.search(\n",
    "            collections=collections, intersects = point)\n",
    "\n",
    "\n",
    "\n",
    "    # print(f'{search.matched()} Tiles Found...')\n",
    "\n",
    "\n",
    "    item_collection = search.get_all_items()\n",
    "\n",
    "    if limit:\n",
    "        item_collection = item_collection[:limit]\n",
    "\n",
    "    if band:\n",
    "        links = []\n",
    "        if type(band) == list:\n",
    "            for i in item_collection:\n",
    "                for b in band:\n",
    "                    link = i.assets[b].href\n",
    "                    # print(link)\n",
    "                    links.append(link)\n",
    "        \n",
    "        else:\n",
    "            for i in item_collection:\n",
    "                link = i.assets[band].href\n",
    "                links.append(link)\n",
    "    \n",
    "    else:\n",
    "        links =[]\n",
    "        for i in item_collection:\n",
    "            # print(i.assets)\n",
    "            for key in i.assets:\n",
    "                if key.startswith('B'):\n",
    "                    # link = i.assets[key].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://')\n",
    "                    link = i.assets[key].href\n",
    "\n",
    "                    # print(link)\n",
    "                    links.append(link)\n",
    "\n",
    "    return links\n",
    "\n",
    "def datestdtojd (stddate):\n",
    "    fmt='%Y-%m-%d'\n",
    "    sdtdate = datetime.datetime.strptime(stddate, fmt)\n",
    "    sdtdate = sdtdate.timetuple()\n",
    "    jdate = sdtdate.tm_yday\n",
    "    return(stddate[:4]+str(jdate))\n",
    "\n",
    "def jdtodatestd (jdate):\n",
    "    fmt = '%Y%j'\n",
    "    datestd = datetime.datetime.strptime(jdate, fmt).date()\n",
    "    out = datestd.strftime( '%Y-%m-%d')\n",
    "    return out\n",
    "\n",
    "\n",
    "def ssc_json_gen(aquasat_path, use_restart = True):\n",
    "\n",
    "    # read in aquasat\n",
    "    df = pd.read_csv(aquasat_path)\n",
    "\n",
    "    f = IntProgress(min=0, max=len(df)) # instantiate the bar\n",
    "    display(f) # display the bar\n",
    "\n",
    "    fails = 0\n",
    "\n",
    "    if use_restart:\n",
    "        # needs to be more robust restarting\n",
    "        the_range = range(len(glob.glob('./ssc_jsons/*'))+1,len(df))\n",
    "    else:\n",
    "        the_range = range(len(df))\n",
    "\n",
    "    f.value += min(the_range)\n",
    "    \n",
    "    for i in the_range:\n",
    "        lat, lon, date = str(df.iloc[i]['lat']), str(df.iloc[i]['long']), df.iloc[0]['date']\n",
    "        try:\n",
    "            # date = datestdtojd('2013-01-13')\n",
    "            # print(date)\n",
    "            links = find_hls_tiles(point=[lon, lat])\n",
    "            if len(links)>0:\n",
    "                date_list = []\n",
    "                for l in links:\n",
    "                    date = l.split('/')[-1].split('.')[3][:7]\n",
    "                    date_list.append(jdtodatestd(date))\n",
    "\n",
    "                # date_list = [jdtodatestd(x) for x in date_list]\n",
    "\n",
    "                # date_list.sort(key=lambda date: datetime.datetime.strptime(date, '%Y-%m-%d'))\n",
    "\n",
    "\n",
    "                ssc_dict = {}\n",
    "\n",
    "                date_dict = {}\n",
    "                for z in list(set(date_list)):\n",
    "                    \n",
    "\n",
    "                    date_list = np.asarray(date_list)\n",
    "                    \n",
    "                    bool_arr = date_list==z\n",
    "\n",
    "                    link_list_for_date = np.asarray(links)[bool_arr]\n",
    "\n",
    "                    link_list_for_date = sorted(link_list_for_date, key = lambda x: x.split('.')[-2][1:])\n",
    "                    date_dict[z] = link_list_for_date\n",
    "\n",
    "                ssc_dict[str(lon) +','+ str(lat)]  = date_dict\n",
    "\n",
    "\n",
    "                # Serializing json\n",
    "                json_object = json.dumps(ssc_dict, indent=4)\n",
    "                \n",
    "                # Writing to sample.json\n",
    "                with open(f\"./ssc_jsons/date_dict_{i}.json\", \"w\") as outfile:\n",
    "                    outfile.write(json_object)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('fail')\n",
    "            fails +=1\n",
    "        f.value += 1 # signal to increment the progress bar\n",
    "        time.sleep(.1)\n",
    "    print(f'Total fails: {fails} ')\n",
    "    print(f'Total Success: {len(df)-fails}')\n",
    "    return ssc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/02_Data_Discovery_CMR-STAC_API.html\n",
    "\n",
    "# https://rafatieppo.github.io/post/2018_12_01_juliandate/\n",
    "\n",
    "from pystac_client import Client  \n",
    "from collections import defaultdict    \n",
    "import json\n",
    "import geopandas\n",
    "from cartopy import crs\n",
    "import geopandas as gpd\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "import netCDF4\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import glob\n",
    "import asyncio\n",
    "\n",
    "aqua_path = '/data/data/downloads/Aqusat_TSS_v1.csv'\n",
    "\n",
    "# jj = ssc_json_gen(aqua_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find hls tiles given a point\n",
    "\n",
    "def find_hls_tiles(point=False, band=False, limit=False, collections = ['HLSL30.v2.0', 'HLSS30.v2.0'], date_range = False):\n",
    "\n",
    "    STAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "\n",
    "\n",
    "    catalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        x, y = point[0], point[1]\n",
    "        # print(x,y)\n",
    "    except TypeError:\n",
    "        print(\"Point must be in the form of [lon,lat]\")\n",
    "        raise\n",
    "\n",
    "    point = geopandas.points_from_xy([x],[y])\n",
    "    point = point[0]\n",
    "\n",
    "\n",
    "\n",
    "    # JOHN - THIS IS WHERE YOU WOULD ADD IN SEARCH PARAMETERS\n",
    "    if date_range:\n",
    "\n",
    "        search = catalog.search(\n",
    "            collections=collections, intersects = point, datetime=date_range)\n",
    "    else:\n",
    "        search = catalog.search(\n",
    "            collections=collections, intersects = point)\n",
    "\n",
    "\n",
    "\n",
    "    # print(f'{search.matched()} Tiles Found...')\n",
    "\n",
    "\n",
    "    item_collection = search.get_all_items()\n",
    "\n",
    "    if limit:\n",
    "        item_collection = item_collection[:limit]\n",
    "\n",
    "    if band:\n",
    "        links = []\n",
    "        if type(band) == list:\n",
    "            for i in item_collection:\n",
    "                for b in band:\n",
    "                    link = i.assets[b].href\n",
    "                    # print(link)\n",
    "                    links.append(link)\n",
    "        \n",
    "        else:\n",
    "            for i in item_collection:\n",
    "                link = i.assets[band].href\n",
    "                links.append(link)\n",
    "    \n",
    "    else:\n",
    "        links =[]\n",
    "        for i in item_collection:\n",
    "            # print(i.assets)\n",
    "            for key in i.assets:\n",
    "                if key.startswith('B'):\n",
    "                    # link = i.assets[key].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://')\n",
    "                    link = i.assets[key].href\n",
    "\n",
    "                    # print(link)\n",
    "                    links.append(link)\n",
    "\n",
    "    return links\n",
    "\n",
    "def datestdtojd (stddate):\n",
    "    fmt='%Y-%m-%d'\n",
    "    sdtdate = datetime.datetime.strptime(stddate, fmt)\n",
    "    sdtdate = sdtdate.timetuple()\n",
    "    jdate = sdtdate.tm_yday\n",
    "    return(stddate[:4]+str(jdate))\n",
    "\n",
    "def jdtodatestd (jdate):\n",
    "    fmt = '%Y%j'\n",
    "    datestd = datetime.datetime.strptime(jdate, fmt).date()\n",
    "    out = datestd.strftime( '%Y-%m-%d')\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "async def find_tiles_and_write(df, i):\n",
    "\n",
    "    lat, lon, date = str(df.iloc[i]['lat']), str(df.iloc[i]['long']), df.iloc[0]['date']\n",
    "    try:\n",
    "        links = find_hls_tiles(point=[lon, lat])\n",
    "        if len(links)>0:\n",
    "            date_list = []\n",
    "            for l in links:\n",
    "                date = l.split('/')[-1].split('.')[3][:7]\n",
    "                date_list.append(jdtodatestd(date))\n",
    "            ssc_dict = {}\n",
    "            date_dict = {}\n",
    "            for z in list(set(date_list)):\n",
    "\n",
    "                date_list = np.asarray(date_list)\n",
    "                \n",
    "                bool_arr = date_list==z\n",
    "\n",
    "                link_list_for_date = np.asarray(links)[bool_arr]\n",
    "\n",
    "                link_list_for_date = sorted(link_list_for_date, key = lambda x: x.split('.')[-2][1:])\n",
    "                date_dict[z] = link_list_for_date\n",
    "\n",
    "            ssc_dict[str(lon) +','+ str(lat)]  = date_dict\n",
    "            # Serializing json\n",
    "            json_object = json.dumps(ssc_dict, indent=4)\n",
    "            \n",
    "            # Writing to sample.json\n",
    "            with open(f\"./ssc_jsons/date_dict_{i}.json\", \"w\") as outfile:\n",
    "                outfile.write(json_object)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('fail')\n",
    "        fails +=1\n",
    "\n",
    "\n",
    "\n",
    "# turn this function async\n",
    "\n",
    "\n",
    "async def driver(df, the_range):\n",
    "    await asyncio.gather(*(find_tiles_and_write(i) for i in the_range))\n",
    "\n",
    "def ssc_json_gen(aquasat_path, use_restart = True):\n",
    "\n",
    "    # read in aquasat\n",
    "    df = pd.read_csv(aquasat_path)\n",
    "\n",
    "    fails = 0\n",
    "\n",
    "    if use_restart:\n",
    "        # needs to be more robust restarting\n",
    "        the_range = range(len(glob.glob('./ssc_jsons/*'))+1,len(df))\n",
    "    else:\n",
    "        the_range = range(len(df))\n",
    "    \n",
    "\n",
    "    # make new function here that we want to parallelize\n",
    "    # for i in the_range:\n",
    "    #     find_tiles_and_write(df, i)\n",
    "\n",
    "    asyncio.run(driver(df, the_range=the_range))\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Total fails: {fails} ')\n",
    "    print(f'Total Success: {len(df)-fails}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/02_Data_Discovery_CMR-STAC_API.html\n",
    "\n",
    "# https://rafatieppo.github.io/post/2018_12_01_juliandate/\n",
    "\n",
    "from pystac_client import Client  \n",
    "from collections import defaultdict    \n",
    "import json\n",
    "import geopandas\n",
    "from cartopy import crs\n",
    "import geopandas as gpd\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "import netCDF4\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import glob\n",
    "import asyncio\n",
    "\n",
    "aqua_path = '/data/data/downloads/Aqusat_TSS_v1.csv'\n",
    "\n",
    "jj = ssc_json_gen(aqua_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hlstutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ba854cefeea420105352005a7fd47290ef8212ad72b2335f23a5c2f1b863bf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
